# Agri-MÃ©tÃ©o Big Data Pipeline

Pipeline ETL (Extract, Transform, Load) scalable pour collecter, traiter et analyser des donnÃ©es **mÃ©tÃ©orologiques** et **agricoles** Ã  grande Ã©chelle.  
L'objectif : fournir des insights actionnables pour optimiser les rendements agricoles en fonction des conditions climatiques (tempÃ©rature, prÃ©cipitations, humiditÃ©, etc.).

---

## Logique du Pipeline (Vision MÃ©tier)

Le projet suit une architecture classique ETL :

1. **Extract** : Collecte automatisÃ©e via APIs (OpenWeatherMap, FAO, Copernicus, etc.) et fichiers historiques.
2. **Transform** : Nettoyage, enrichissement et agrÃ©gation avec **PySpark** pour traiter de gros volumes de donnÃ©es.
3. **Load** : Stockage optimisÃ© (format Parquet, Delta Lake) et/ou insertion dans une base de donnÃ©es (PostgreSQL, BigQueryâ€¦).
4. **Visualisation** : Dashboard interactif (Streamlit, Power BI, Tableau) pour suivre les indicateurs clÃ©s.

---

### DÃ©pendances 
Toutes les dÃ©pendances sont listÃ©es dans `requirements.txt`.
    pyspark>=3.4.0
    pandas>=2.0
    requests>=2.28
    pyyaml>=6.0
    python-dotenv>=1.0
    sqlalchemy>=2.0
    psycopg2-binary>=2.9  

## Structure du Projet

agri-meteo-bigdata-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # DonnÃ©es brutes
â”‚   â”œâ”€â”€ processed/           # DonnÃ©es nettoyÃ©es
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py           # Collecte donnÃ©es (API / CSV)
â”‚   â”œâ”€â”€ transform.py         # Nettoyage & agrÃ©gation (PySpark)
â”‚   â”œâ”€â”€ load.py              # Insertion DB / Parquet
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb    # Analyse exploratoire (optionnel)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ screenshots/         # Images Power BI / Tableau
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Config API / DB
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


## Installation
    - git clone https://github.com/biko2020/agri-meteo-bigdata-pipeline.git
    - cd agri-meteo-bigdata-pipeline

## CrÃ©er un environnement virtuel (recommandÃ©)

# Linux / macOS
    python -m venv .venv
    source .venv/bin/activate

 # Windows
    python -m venv .venv
    .venv\Scripts\activate


## Installer les dÃ©pendances
pip install --upgrade pip
pip install -r requirements.txt


## -***- ExÃ©cution du projet -***-

 - # ExÃ©cution locale simple
    # 1. Collecte des donnÃ©es 
    python scripts/extract.py

    # 2. Transformation (PySpark)
    spark-submit scripts/transform.py

    # 3. Chargement
    python scripts/load.py
    
 - # ExÃ©cution Utiliser Docker
    `Lancer l'ensemble des services (Spark + PostgreSQL + Jupyter):`
    docker-compose up -d
    docker exec -it spark-master bash
    spark-submit /app/scripts/transform.py

- # ExÃ©cution Utiliser Jupyter Notebook
    jupyter lab notebooks/exploration.ipynb
    `Ou via Docker :`
    docker-compose up jupyter
    http://localhost:8888

---


## ðŸ‘¤ Contact
    **AIT OUFKIR BRAHIM** 
    Email : aitoufkirbrahimab@gmail.com
    ðŸ”— GitHub : https://github.com/biko2020/agri-meteo-bigdata-pipeline