# ğŸŒ¾ Agri-Meteo Big Data Pipeline

> Pipeline ETL Big Data automatisÃ© pour collecter, transformer et analyser des donnÃ©es mÃ©tÃ©orologiques avec **Airflow, Spark, PostgreSQL et Metabase**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Apache Spark 3.4.1](https://img.shields.io/badge/spark-3.4.1-orange.svg)](https://spark.apache.org/)
[![Apache Airflow 2.8.1](https://img.shields.io/badge/airflow-2.8.1-green.svg)](https://airflow.apache.org/)

---

## ğŸ¯ Vision MÃ©tier

Les donnÃ©es mÃ©tÃ©orologiques sont souvent dispersÃ©es, volumineuses et difficiles Ã  exploiter. Ce projet fournit une **chaÃ®ne de traitement complÃ¨te** et **entiÃ¨rement automatisÃ©e** transformant les donnÃ©es brutes en **KPI exploitables**, prÃªts pour la Business Intelligence et l'aide Ã  la dÃ©cision agricole.

**ProblÃ©matique rÃ©solue :** Comment anticiper les risques climatiques (sÃ©cheresse, inondations, stress thermique) pour optimiser la gestion agricole ?

---

## ğŸ—ï¸ Architecture du Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open-Meteo API   â”‚  â† Source de donnÃ©es
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Airflow     â”‚  â† Orchestration (@daily)
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Extract       â”‚ â”‚  â†’ RÃ©cupÃ©ration donnÃ©es mÃ©tÃ©o
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Transform     â”‚ â”‚  â†’ Nettoyage + Calculs KPI (Spark)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Load          â”‚ â”‚  â†’ Chargement PostgreSQL + Parquet
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data  â”‚   â”‚ PostgreSQL â”‚
â”‚  Lake  â”‚   â”‚   (OLAP)   â”‚
â”‚(Parquet)â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Visualisation BI     â”‚
              â”‚  â€¢ Metabase           â”‚
              â”‚  â€¢ Superset (opt.)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architecture microservices** : 5 conteneurs Docker orchestrÃ©s

---

## ğŸ› ï¸ Stack Technique

### Core Technologies
- **Orchestration** : Apache Airflow 2.8.1 (LocalExecutor)
- **Big Data Processing** : Apache Spark 3.4.1 (PySpark)
- **Database** : PostgreSQL 15 (OLAP + Airflow metadata)
- **Data Visualization** : Metabase + Apache Superset
- **Containerization** : Docker & Docker Compose
- **Languages** : Python 3.8+, SQL

### Libraries & Tools
- **ETL** : PySpark, Pandas, SQLAlchemy
- **Data Formats** : Parquet (Data Lake), CSV (staging), JDBC (PostgreSQL)
- **API** : Open-Meteo API (weather data)
- **Networking** : Docker custom network (agri-network)

---

## âš™ï¸ FonctionnalitÃ©s ClÃ©s

âœ… **Automatisation complÃ¨te** â€“ Pipeline ETL exÃ©cutÃ© quotidiennement (@daily scheduling)  
âœ… **Traitement distribuÃ©** â€“ Apache Spark pour gÃ©rer des volumes importants  
âœ… **KPI MÃ©tier** â€“ Calcul automatique de risques climatiques (HIGH/MEDIUM/LOW)  
âœ… **Dual Storage** â€“ Data Lake (Parquet) + Data Warehouse (PostgreSQL)  
âœ… **Monitoring** â€“ Spark UI + Airflow UI pour supervision  
âœ… **Dashboards interactifs** â€“ Visualisations temps rÃ©el avec Metabase  
âœ… **ScalabilitÃ©** â€“ Architecture prÃªte pour multi-rÃ©gions et volumes croissants

---

## ğŸ“Š Indicateurs & KPI Produits

### MÃ©triques CalculÃ©es
- **TempÃ©rature moyenne quotidienne** (Â°C)
- **PrÃ©cipitations cumulÃ©es** (mm)
- **Risque climatique** (HIGH/MEDIUM/LOW)
- **Tendances et corrÃ©lations** tempÃ©rature-prÃ©cipitations

### RÃ¨gles MÃ©tier (Climate Risk KPI)
| PrÃ©cipitations | Niveau de Risque | Impact Agricole |
|----------------|------------------|-----------------|
| < 1 mm         | ğŸ”´ **HIGH**      | SÃ©cheresse - Irrigation urgente |
| 1-5 mm         | ğŸŸ¡ **MEDIUM**    | ModÃ©rÃ© - Surveillance |
| 5-10 mm        | ğŸŸ¢ **LOW**       | Optimal |
| > 10 mm        | ğŸ”´ **HIGH**      | Inondation - Drainage nÃ©cessaire |

---

## ğŸ“¸ Captures d'Ã©cran

### Dashboard Metabase
![Dashboard Overview](dashboard/screenshots/01-metabase-dashboard-overview.png)
*Dashboard complet : TempÃ©rature, PrÃ©cipitations et CorrÃ©lations*

### Pipeline Airflow (SuccÃ¨s)
![Airflow DAG Success](dashboard/screenshots/05-airflow-dag-success.png)
*Pipeline ETL exÃ©cutÃ© avec succÃ¨s - 3 tÃ¢ches validÃ©es*

### Analyses DÃ©taillÃ©es
<table>
  <tr>
    <td width="50%">
      <img src="dashboard/screenshots/02-temperature-bar-chart.png" alt="Temperature Chart"/>
      <p align="center"><i>TempÃ©rature moyenne par jour</i></p>
    </td>
    <td width="50%">
      <img src="dashboard/screenshots/04-trends-correlation.png" alt="Correlation Chart"/>
      <p align="center"><i>CorrÃ©lation TempÃ©rature/PrÃ©cipitations</i></p>
    </td>
  </tr>
</table>

---

## ğŸ“ Structure du Projet
```
agri-meteo-bigdata-pipeline/
â”‚
â”œâ”€â”€ airflow/                        # Orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ agri_meteo_etl.py      # DAG principal (@daily)
â”‚   â”œâ”€â”€ logs/                       # Logs Airflow
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ Dockerfile                  # Custom Airflow + Docker CLI
â”‚   â””â”€â”€ docker-compose.airflow.yml
â”‚
â”œâ”€â”€ docker/                         # Infrastructure
â”‚   â”œâ”€â”€ Dockerfile                  # Spark + PostgreSQL JDBC
â”‚   â”œâ”€â”€ docker-compose.yml          # Spark, Postgres, Metabase
â”‚   â””â”€â”€ requirements.txt            # DÃ©pendances Python
â”‚
â”œâ”€â”€ scripts/                        # Scripts ETL
â”‚   â”œâ”€â”€ extract.py                  # Extraction API Open-Meteo
â”‚   â”œâ”€â”€ transform.py                # Transformation PySpark
â”‚   â””â”€â”€ load.py                     # Chargement PostgreSQL
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # CSV bruts (staging)
â”‚   â””â”€â”€ processed/                  # Parquet partitionnÃ© (Data Lake)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ screenshots/                # Captures Metabase
â”‚
â”œâ”€â”€ notebooks/                      # (Optionnel) Exploration
â”‚   â””â”€â”€ exploration.ipynb
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # Configuration (si nÃ©cessaire)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸš€ Installation & DÃ©marrage

### PrÃ©requis
- **Docker** (version 20.10+) & **Docker Compose** (version 2.0+)
- **4 GB RAM** minimum recommandÃ©
- **Ports disponibles** : 3000, 5432, 7077, 8080, 8081, 8088

### 1ï¸âƒ£ Cloner le Projet
```bash
git clone https://github.com/biko2020/agri-meteo-bigdata-pipeline.git
cd agri-meteo-bigdata-pipeline
```

### 2ï¸âƒ£ CrÃ©er le RÃ©seau Docker
```bash
docker network create agri-network
```

### 3ï¸âƒ£ DÃ©marrer les Services
```bash
# Lancer Spark, PostgreSQL et Metabase
docker-compose -f docker/docker-compose.yml up -d

# Lancer Airflow (orchestration)
docker-compose -f airflow/docker-compose.airflow.yml up -d

# VÃ©rifier que tous les conteneurs sont actifs
docker ps
```

### 4ï¸âƒ£ AccÃ©der aux Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow** | http://localhost:8081 | admin / admin |
| **Metabase** | http://localhost:3000 | (premiÃ¨re connexion) |
| **Spark UI** | http://localhost:8080 | - |
| **Superset** (opt.) | http://localhost:8088 | admin / admin |

### 5ï¸âƒ£ ExÃ©cuter le Pipeline

#### Option A : Via Airflow UI (RecommandÃ©)
1. AccÃ©dez Ã  http://localhost:8081
2. Activez le DAG `agri_meteo_etl`
3. Cliquez sur le bouton **Play** pour dÃ©clencher manuellement
4. Surveillez l'exÃ©cution (3 tÃ¢ches : extract â†’ transform â†’ load)

#### Option B : Manuellement (pour tests)
```bash
# Extraction
docker exec spark python3 /app/scripts/extract.py

# Transformation (Spark)
docker exec spark spark-submit /app/scripts/transform.py

# Chargement PostgreSQL
docker exec spark python3 /app/scripts/load.py
```

### 6ï¸âƒ£ VÃ©rifier les DonnÃ©es
```bash
# Connexion PostgreSQL
docker exec -it postgres psql -U postgres -d agri_db

# Lister les tables
\dt

# Afficher les donnÃ©es
SELECT * FROM weather_kpi ORDER BY date;
```

---

## ğŸ”„ Flux du Pipeline ETL

### 1. **Extract** (`extract.py`)
- **Source** : API Open-Meteo (latitude: 33.6, longitude: -7.6 - Casablanca)
- **DonnÃ©es** : TempÃ©rature max, PrÃ©cipitations (7 jours)
- **Output** : `/app/data/raw/weather.csv`
- **DurÃ©e** : ~30 secondes

### 2. **Transform** (`transform.py`)
- **Engine** : PySpark (traitement distribuÃ©)
- **OpÃ©rations** :
  - Nettoyage (dropna)
  - AgrÃ©gations (avg tempÃ©rature, avg prÃ©cipitations)
  - Calcul KPI `climate_risk` (HIGH/MEDIUM/LOW)
- **Output** : 
  - Parquet partitionnÃ© â†’ `/app/data/processed/weather/`
  - PostgreSQL â†’ table `weather_kpi` (via JDBC)
- **DurÃ©e** : ~1-2 minutes

### 3. **Load** (`load.py`)
- **Source** : Parquet
- **Target** : PostgreSQL (`agri_db.weather_kpi`)
- **Method** : SQLAlchemy bulk insert
- **DurÃ©e** : ~30 secondes

**â±ï¸ Temps total du pipeline : 2-5 minutes**

---

## ğŸ“ˆ Configuration Metabase

### Connexion Ã  la Base de DonnÃ©es

1. AccÃ©dez Ã  Metabase : http://localhost:3000
2. **Admin** â†’ **Databases** â†’ **Add database**
3. Configurez :
   - **Database type** : PostgreSQL
   - **Name** : AgriMeteo
   - **Host** : `postgres`
   - **Port** : `5432`
   - **Database name** : `agri_db`
   - **Username** : `postgres`
   - **Password** : `postgres`

4. Cliquez sur **Save** puis **Sync database schema now**

### CrÃ©er le Dashboard

Consultez les captures d'Ã©cran dans `dashboard/screenshots/` pour reproduire :
- Bar Chart : TempÃ©rature moyenne
- Table : PrÃ©cipitations quotidiennes
- Line Chart : CorrÃ©lation tempÃ©rature/prÃ©cipitations

---

## ğŸ’¼ CompÃ©tences DÃ©montrÃ©es

Ce projet illustre une maÃ®trise complÃ¨te de :

### Data Engineering
- Architecture ETL moderne (Extract-Transform-Load)
- Traitement Big Data distribuÃ© (Spark)
- ModÃ©lisation de donnÃ©es (Star Schema OLAP)
- Data Lake vs Data Warehouse

### DevOps & Infrastructure
- Conteneurisation Docker
- Orchestration multi-services (Docker Compose)
- Networking (custom Docker networks)
- CI/CD ready (automatisation complÃ¨te)

### Orchestration & Automation
- Apache Airflow (DAGs, scheduling, monitoring)
- Gestion des dÃ©pendances entre tÃ¢ches
- Retry policies et error handling

### Business Intelligence
- Dashboarding interactif (Metabase)
- Calcul de KPI mÃ©tier
- Data storytelling

---

## ğŸ“ Cas d'Usage Professionnels

Ce pipeline est directement applicable pour :

âœ… **Agriculture** â€“ Optimisation irrigation, prÃ©vision rÃ©coltes  
âœ… **Ã‰nergie** â€“ PrÃ©vision production solaire/Ã©olienne  
âœ… **Assurance** â€“ Ã‰valuation risques climatiques  
âœ… **Supply Chain** â€“ Anticipation perturbations mÃ©tÃ©o  
âœ… **Smart Cities** â€“ Gestion ressources urbaines  

---

## ğŸ”® AmÃ©liorations Futures

### Court Terme
- [ ] Tests unitaires (pytest) + tests d'intÃ©gration
- [ ] Logging avancÃ© (ELK stack)
- [ ] Alerting automatique (email/Slack) sur seuils KPI

### Moyen Terme
- [ ] Machine Learning : PrÃ©visions mÃ©tÃ©o J+7 (Prophet, LSTM)
- [ ] API REST (FastAPI) pour consommation externe
- [ ] Multi-rÃ©gions : Scraping de 10+ villes
- [ ] CDC (Change Data Capture) pour mises Ã  jour incrÃ©mentales

### Long Terme
- [ ] Migration Cloud (AWS EMR / GCP Dataproc)
- [ ] Streaming temps rÃ©el (Kafka + Spark Streaming)
- [ ] Data Quality monitoring (Great Expectations)
- [ ] Monitoring infra (Prometheus + Grafana)

---

## ğŸ“¦ DÃ©pendances Python
```txt
# Big Data & Processing
pyspark==3.4.1

# Data Manipulation
pandas
pyarrow
fastparquet

# Database
sqlalchemy
psycopg2-binary

# API & Utils
requests
pyyaml
```

---

## ğŸ› Troubleshooting

### Erreur : "Cannot connect to Docker daemon"
**Solution** : Assurez-vous que Docker Desktop est dÃ©marrÃ© et que `/var/run/docker.sock` est montÃ© dans Airflow.

### Erreur : "ClassNotFoundException: org.postgresql.Driver"
**Solution** : Reconstruisez le conteneur Spark avec `docker-compose -f docker/docker-compose.yml build`

### Metabase ne voit pas les tables
**Solution** : Cliquez sur "Sync database schema now" dans Admin â†’ Databases aprÃ¨s l'exÃ©cution du pipeline.

### Pipeline lent (>10 minutes)
**Solution** : Augmentez les ressources Docker (4 GB RAM minimum recommandÃ©).

---

## ğŸ“„ Licence

MIT License - Libre d'utilisation pour projets personnels et commerciaux.

---

## ğŸ‘¤ Auteur

**AIT OUFKIR BRAHIM**  
*Data Engineer | Big Data Developer*

- ğŸ“§ Email : [aitoufkirbrahimab@gmail.com](mailto:aitoufkirbrahimab@gmail.com)
- ğŸ’¼ LinkedIn : [linkedin.com/in/brahim-ait-oufkir](https://linkedin.com/in/brahim-ait-oufkir)
- ğŸ™ GitHub : [github.com/biko2020](https://github.com/biko2020)

---
