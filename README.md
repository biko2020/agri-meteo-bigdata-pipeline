# ğŸŒ¾ Agri-Meteo Big Data Pipeline

Pipeline **Big Data / ETL (Extract, Transform, Load)** pour collecter, nettoyer, transformer et analyser des donnÃ©es **mÃ©tÃ©orologiques** (extensible aux donnÃ©es agricoles), afin de produire des **indicateurs dÃ©cisionnels exploitables**.

ğŸ¯ **Objectif du projet**  
DÃ©montrer la mise en place dâ€™un pipeline de donnÃ©es **scalable, reproductible et orientÃ© production**, prÃªt pour des **cas dâ€™usage professionnels et missions freelance** (Data Engineering / Big Data / BI).

---

## ğŸ¯ Vision MÃ©tier

Dans de nombreux secteurs (agriculture, Ã©nergie, environnement, logistique), les donnÃ©es mÃ©tÃ©o sont :
- dispersÃ©es (APIs, fichiers CSV),
- volumineuses et hÃ©tÃ©rogÃ¨nes,
- difficiles Ã  exploiter directement par les dÃ©cideurs.

Ce projet fournit une **chaÃ®ne de traitement complÃ¨te** permettant de transformer ces donnÃ©es brutes en **KPI clairs et exploitables**, prÃªts Ã  Ãªtre visualisÃ©s dans des outils de Business Intelligence.

---

## ğŸ§  Logique du Pipeline (ETL)

Le projet suit une architecture ETL classique, orientÃ©e production :

1. **Extract**
   - Collecte automatisÃ©e des donnÃ©es mÃ©tÃ©o via API publique
   - Stockage des donnÃ©es brutes (*raw data*)

2. **Transform**
   - Nettoyage des donnÃ©es (formats, valeurs manquantes)
   - AgrÃ©gation et calcul dâ€™indicateurs avec **PySpark**

3. **Load**
   - Stockage optimisÃ© au format **Parquet**
   - Insertion dans une base **PostgreSQL**

4. **Exploitation**
   - DonnÃ©es prÃªtes pour la visualisation (Power BI, Tableau, etc.)

---

## ğŸ—ï¸ Architecture globale

API / CSV
â†“
Python (Extract)
â†“
PySpark (Transform)
â†“
Parquet / PostgreSQL
â†“
Dashboard BI


---

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python**
- **PySpark**
- **Pandas**
- **SQL / PostgreSQL**
- **Docker & Docker Compose**
- **Power BI / Tableau** (visualisation)

---

## ğŸ“Š Exemples dâ€™indicateurs produits

- TempÃ©rature moyenne par pÃ©riode
- Cumul des prÃ©cipitations
- Tendances saisonniÃ¨res
- DonnÃ©es prÃªtes pour analyses mÃ©tier ou corrÃ©lations ultÃ©rieures

---

## ğŸ“¦ DÃ©pendances

Toutes les dÃ©pendances Python sont listÃ©es dans `requirements.txt` :
pyspark
pandas
requests
pyyaml
sqlalchemy
psycopg2-binary


---

## ğŸ“ Structure du projet

agri-meteo-bigdata-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # DonnÃ©es brutes
â”‚ â””â”€â”€ processed/ # DonnÃ©es transformÃ©es
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract.py # Extraction des donnÃ©es
â”‚ â”œâ”€â”€ transform.py # Nettoyage & agrÃ©gation (PySpark)
â”‚ â””â”€â”€ load.py # Chargement vers DB / Parquet
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ exploration.ipynb # Analyse exploratoire
â”‚
â”œâ”€â”€ dashboard/
â”‚ â””â”€â”€ screenshots/ # Captures des dashboards
â”‚
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ config.yaml # Configuration API / DB
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


---

## âš™ï¸ Installation (Docker â€“ recommandÃ©)

### 1ï¸âƒ£ Cloner le projet

```bash
git clone https://github.com/biko2020/agri-meteo-bigdata-pipeline.git
cd agri-meteo-bigdata-pipeline

### 2ï¸âƒ£ Lancer lâ€™environnement complet avec Docker
docker compose up -d
docker ps


â–¶ï¸ ExÃ©cution du pipeline avec Docker
   # AccÃ©der au conteneur Spark
   docker exec -it spark-master bash 

   # 1. Extraction
   python /app/scripts/extract.py

   # 2. Transformation (PySpark)
   spark-submit /app/scripts/transform.py

   # 3. Chargement
   python /app/scripts/load.py


###  Cas dâ€™usage 

* Ce pipeline est directement applicable Ã  des missions telles que :

* crÃ©ation de pipelines ETL,

* traitement de donnÃ©es volumineuses,

* migration CSV / Excel vers bases de donnÃ©es,

* prÃ©paration de donnÃ©es pour dashboards BI,

* projets Data Engineering / Big Data.


### Contact

** AIT OUFKIR BRAHIM
** Data Engineer / Big Data Developer

   - Email : aitoufkirbrahimab@gmail.com
   - GitHub : https://github.com/biko2020/agri-meteo-bigdata-pipeline